{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GUI CS31-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVKQ5FMSo15Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c9f4a2-1c39-4970-a8eb-72e47771ba52"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Dec 12 09:26:42 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE5zs00yL8gD"
      },
      "source": [
        "# Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyiGmcWPLNbh"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "#tf denpendencies\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import Dense, LSTM,Dropout,Activation,Input,Bidirectional\n",
        "from tensorflow.keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
        "from tensorflow.keras.models import load_model,save_model\n",
        "from tensorflow.keras.utils import plot_model,to_categorical\n",
        "\n",
        "#plotly\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQYtk5d1LkpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee4dc30-6f45-42bc-8b50-6688f4737d0f"
      },
      "source": [
        "if torch.cuda.is_available():      \n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEUMMgfLJFhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a07070c-7a31-499c-bb41-750d32d6079f"
      },
      "source": [
        "#install streamlit\r\n",
        "!pip install streamlit"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.6/dist-packages (0.72.0)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.6/dist-packages (from streamlit) (3.1.11)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.1.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (7.0.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.5.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from streamlit) (2.8.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (4.1.1)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (3.12.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from streamlit) (20.7)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.6/dist-packages (from streamlit) (2.0.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.14.1)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.18.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.16.6)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.0.1)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (4.1.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from gitpython->streamlit) (4.0.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.0->streamlit) (2018.9)\n",
            "Requirement already satisfied: jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit) (2.11.2)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.5.1)\n",
            "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.4.2)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit) (4.3.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->streamlit) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (50.3.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from validators->streamlit) (4.4.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->streamlit) (2020.12.5)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->gitpython->streamlit) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.10.1->pydeck>=0.1.dev5->streamlit) (1.1.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.0.8)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.7.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (20.0.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.9.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.6.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.4.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.2.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.6.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxcgrId_JXc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c6668d-72a8-4ed2-f6ad-d67035bf09fd"
      },
      "source": [
        "#install pyngrok\r\n",
        "!pip install pyngrok==4.1.1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyngrok==4.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/a9/de2e15c92eb3aa4a2646ce3a7542317eb69ac47f667578ce8bf916320847/pyngrok-4.1.1.tar.gz\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyngrok==4.1.1) (0.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyngrok==4.1.1) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-cp36-none-any.whl size=15970 sha256=62cd0c034fcd325d2c824555805f40b8a24905968486d1c661fea2d0ac03e5cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/71/0d/1695f7c8815c0beb3b5d9b35d6eec9243c87e6070fbe3977fa\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fivyepk8FEL7"
      },
      "source": [
        "#install plotly\n",
        "!pip install plotly==4.7.1\n",
        "!wget https://github.com/plotly/orca/releases/download/v1.2.1/orca-1.2.1-x86_64.AppImage -O /usr/local/bin/orca\n",
        "!chmod +x /usr/local/bin/orca\n",
        "!apt-get install xvfb libgtk2.0-0 libgconf-2-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufVu61NGMvgN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f379bf70-dc9a-4d92-fd2e-d7170fee31ee"
      },
      "source": [
        "#install scikit-tensor\r\n",
        "!pip install scikit-tensor-py3"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-tensor-py3 in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: numpy==1.16.* in /usr/local/lib/python3.6/dist-packages (from scikit-tensor-py3) (1.16.6)\n",
            "Requirement already satisfied: scipy==1.3.* in /usr/local/lib/python3.6/dist-packages (from scikit-tensor-py3) (1.3.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N26fAa7roZCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d696f66b-4694-4e11-a21e-dccf4aca0581"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwKnFdGDTYzm"
      },
      "source": [
        "# Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_JykrFvzc7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fab4fe7-4953-4d79-9255-6eb81f3178f0"
      },
      "source": [
        "%%writefile utils.py\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import joblib\n",
        "import os\n",
        "import random\n",
        "\n",
        "#tf denpendencies\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import Dense, LSTM,Dropout,Activation,Input,Bidirectional\n",
        "from tensorflow.keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
        "from tensorflow.keras.models import load_model,save_model\n",
        "from tensorflow.keras.utils import plot_model,to_categorical\n",
        "from sktensor import dtensor\n",
        "\n",
        "\n",
        "def load_dataset(df_file_path,route_dir,user_dir):\n",
        "  # Load Raw Data\n",
        "  cols = ['id', 'userId', 'name', 'city', 'gender', 'sport',\n",
        "          'calories', 'Route_id', 'distance_adjusted_sum','altitude_adjusted','distance_adjusted','speed_adjusted','heart_rate_adjusted','derived_speed','longitude','latitude']\n",
        "  df = pd.read_csv(df_file_path, usecols=cols)\n",
        "  df.rename(columns={'speed_adjusted': 'speed','altitude_adjusted':'altitude','distance_adjusted':'derived_distance','heart_rate_adjusted':'heart_rate'},inplace=True)\n",
        "  scaler_dic = dict()\n",
        "  features = ['calories','altitude','derived_distance','speed','heart_rate','distance']\n",
        "  # Load Min-max scaler models\n",
        "  dir = '/content/gdrive/My Drive/COMP5703/scaler_model'\n",
        "  # https://drive.google.com/drive/folders/1Q9act2dsQ6lfau-PYw7pXVKVlO4Q22IB?usp=sharing\n",
        "\n",
        "  for feature in features:\n",
        "      path = os.path.join(dir, 'scaler_'+feature+'_2.m')\n",
        "      scaler_dic[feature] = joblib.load(path)\n",
        "  # Load onehot encoder for categorical features\n",
        "\n",
        "  dir = '/content/gdrive/My Drive/COMP5703/onehot/OneHotEncoder.m'\n",
        "  # https://drive.google.com/file/d/1wKtDVj4jxk1ViLj1lhzwdXIn82aNAc9h/view?usp=sharing\n",
        "\n",
        "  OneHot_enc = joblib.load(dir)\n",
        "\n",
        "  user_embed_df = joblib.load(user_dir)\n",
        "  route_embed_df = joblib.load(route_dir)\n",
        "\n",
        "  return df,user_embed_df,route_embed_df,OneHot_enc,scaler_dic\n",
        "\n",
        "\n",
        "def data_process(gender, sport, calories, Route_id, distance_adjusted_sum, user_embed, route_embed, scaler_dic,OneHot_enc):\n",
        "    # label encode gender\n",
        "\n",
        "    choices = [0, 1, 2]\n",
        "    conditions = [\n",
        "        (gender == 'male'),\n",
        "        (gender == 'female'),\n",
        "        (gender == 'unknown')]\n",
        "\n",
        "    genderId = np.select(conditions, choices, default=0)\n",
        "\n",
        "    # label encode sport\n",
        "\n",
        "    conditions = [\n",
        "        (sport == 'run'),\n",
        "        (sport == 'bike'),\n",
        "        (sport == 'mountain bike')]\n",
        "\n",
        "    sportId = np.select(conditions, choices, default=0)\n",
        "\n",
        "    # scale calories\n",
        "    calories_scaled = scaler_dic['calories'].transform(\n",
        "        np.array(calories).reshape(-1, 1))[0]\n",
        "    # scale distance\n",
        "    total_distance_scaled = scaler_dic['distance'].transform(\n",
        "        np.array(distance_adjusted_sum).reshape(-1, 1))[0]\n",
        "    # one hot encode genderId and sportId\n",
        "    gender_sport_onehot = OneHot_enc.transform(\n",
        "        np.hstack((genderId, sportId)).reshape(1, -1)).toarray()[0]\n",
        "    # concatenate input features into numpy array\n",
        "    data_input = np.hstack((calories_scaled, total_distance_scaled,\n",
        "                            user_embed, route_embed, gender_sport_onehot))\n",
        "\n",
        "    return data_input\n",
        "\n",
        "def predict_distance(model,data_input,scaler_dic,device):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      pred = model(torch.Tensor(data_input).to(device)).item()\n",
        "\n",
        "  # convert predicted distance to km\n",
        "  pred_km = scaler_dic['distance'].inverse_transform(\n",
        "      np.array(pred).reshape(-1, 1))[0][0]\n",
        "  return pred_km\n",
        "\n",
        "def get_stop_point(pred_distance,derived_distance):\n",
        "  # get where should the workout stop according to the calculated distance from model 1\n",
        "  # if the predicted distance is larger than the whole workout distance, return -1\n",
        "  if pred_distance > sum(derived_distance):\n",
        "    return -1\n",
        "  cur = 0\n",
        "  for i in range(len(derived_distance)):\n",
        "    cur += derived_distance[i]\n",
        "    if cur >= pred_distance:\n",
        "      return i\n",
        "\n",
        "def generate_workout(df,pred_distance,workout_id,user_id,gender,calories,sport):\n",
        "  # generate pd.DataFrame based on the information\n",
        "  # get route\n",
        "  temp = df[df['id'] == workout_id].copy()\n",
        "  temp['userId'] = user_id\n",
        "  temp['gender'] = gender\n",
        "  temp['calories'] = calories\n",
        "  temp['sport'] = sport\n",
        "  temp.reset_index(drop=1,inplace=True)\n",
        "  if len(temp) != 1:\n",
        "    return None\n",
        "  \n",
        "  # Sport Gender Encoding\n",
        "  choices = [0, 1, 2]\n",
        "  conditions = [\n",
        "      (temp['gender'] == 'male'),\n",
        "      (temp['gender'] == 'female'),\n",
        "      (temp['gender'] == 'unknown')]\n",
        "\n",
        "  temp['genderId'] = np.select(conditions, choices, default=0)\n",
        "  conditions = [\n",
        "      (temp['sport'] == 'run'),\n",
        "      (temp['sport'] == 'bike'),\n",
        "      (temp['sport']  == 'mountain bike')]\n",
        "  temp['sportId'] = np.select(conditions, choices, default=0)\n",
        "  # handle contextual data\n",
        "  context_info = ['id','userId','genderId','calories','sportId','Route_id']\n",
        "  context_dict = {col:temp[col][0] for col in context_info}\n",
        "\n",
        "  # handle sequence data\n",
        "  seq = []\n",
        "  speed = eval(temp['speed'][0])\n",
        "  altitude = eval(temp['altitude'][0])\n",
        "  distance = eval(temp['derived_distance'][0])\n",
        "  hr = eval(temp['heart_rate'][0])\n",
        "\n",
        "  complete_id = len(speed)\n",
        "  if  complete_id <499:\n",
        "    sub = 499-complete_id\n",
        "    speed.extend([0]*sub)\n",
        "    hr.extend([0]*sub)\n",
        "\n",
        "  seq = [[altitude[i],distance[i],hr[i],speed[i]] for i in range(499)]\n",
        "  sequence = np.array(seq)\n",
        "  context = np.array([[context_dict['id'],context_dict['userId'],context_dict['genderId'],context_dict['sportId'],context_dict['Route_id'],context_dict['calories']]]*499)\n",
        "  array = np.concatenate((context,\n",
        "                sequence),axis=1)\n",
        "  columns=['workoutId','userId','genderId','sportId','Route_id','calories','altitude','derived_distance','heart_rate','speed']\n",
        "  df_sub = pd.DataFrame(array,columns=columns)\n",
        "  # get stop point\n",
        "  stop_point = get_stop_point(pred_distance,distance)\n",
        "  return df_sub, stop_point\n",
        "\n",
        "def convert_category(value,length):\n",
        "  array_list = [0]*length\n",
        "  array_list[int(value)] = 1\n",
        "  return [array_list]*499\n",
        "\n",
        "def generate_scaled(df_sub,user_embed,route_embed,scaler_dic):\n",
        "  # generate scaled data based on workout dataframe, and return inputs and outputs for model2\n",
        "  features = ['calories','altitude','derived_distance','speed','heart_rate']\n",
        "  gender = convert_category(df_sub['genderId'][0],3)\n",
        "  sport = convert_category(df_sub['sportId'][0],3)\n",
        "  # route = [route_embed_dict[int(df_sub['Route_id'][0])]]*499\n",
        "  # user = [user_embed_dict[int(df_sub['userId'][0])]]*499\n",
        "  route = route_embed.reshape(1,-1).repeat(499,axis=0)\n",
        "  user = user_embed.reshape(1,-1).repeat(499,axis=0)\n",
        "  feature_array = np.concatenate((gender,sport,route,user),axis=1)\n",
        "  for k in features:\n",
        "    # speed input\n",
        "    if k == 'speed':\n",
        "      speed = scaler_dic['speed'].transform(df_sub[k].values.reshape(-1,1))\n",
        "    # heart rate input\n",
        "    elif k == 'heart_rate': \n",
        "      hr = scaler_dic['heart_rate'].transform(df_sub[k].values.reshape(-1,1))\n",
        "    else:\n",
        "      array = scaler_dic[k].transform(df_sub[k].values.reshape(-1,1))\n",
        "      feature_array = np.concatenate((feature_array,array),axis=1)\n",
        "  x1 = feature_array\n",
        "  y1 = speed\n",
        "  y2 = hr\n",
        "  return x1.reshape(1,x1.shape[0],x1.shape[1]),y1.reshape(y1.shape[0],1),y2.reshape(y2.shape[0],1)\n",
        "\n",
        "def apply_stop(stop_stamp):\n",
        "  return np.append(np.ones(stop_stamp),np.zeros(499-stop_stamp))\n",
        "\n",
        "def predict_sp_hr(df,user_embed_df,route_embed_df,scaler_dic,OneHot_enc,device,\n",
        "                  distance_model,SpeedHr_model,\n",
        "                    workout_id,user_id,gender,calories,sport):\n",
        "  # combine model_1 and model_2\n",
        "\n",
        "  # find Route_id based on workout id\n",
        "  Route_id = df.loc[df.id == workout_id, 'Route_id'].to_numpy()[0]\n",
        "  # find route total distance based on workout id\n",
        "  distance_adjusted_sum = df.loc[df.id == workout_id, 'distance_adjusted_sum'].to_numpy()[0]\n",
        "  # find userEmbedding based on userId\n",
        "  user_embed = np.array(\n",
        "      user_embed_df[user_embed_df.userId == user_id].userEmbed.values[0])\n",
        "  # find routeEmbedding based on routeId\n",
        "  route_embed = np.array(\n",
        "      route_embed_df[route_embed_df.Route_id == Route_id].routeEmbed.values[0])\n",
        "  # generate inputs for model_1\n",
        "  input4distance = data_process(gender,sport,calories,Route_id,distance_adjusted_sum,user_embed,route_embed,scaler_dic,OneHot_enc)\n",
        "  # predict distance\n",
        "  pred_dis = predict_distance(distance_model,input4distance,scaler_dic,device)\n",
        "  # generate workout according to the predicted distance\n",
        "  input_df,stop_point =generate_workout(df,pred_distance=pred_dis,workout_id=workout_id,user_id=user_id,gender=gender,sport=sport,calories=calories)\n",
        "  # return -1 if the distance is too short for the target input\n",
        "  if stop_point == -1:\n",
        "    return -1,-1,pred_dis,stop_point\n",
        "  \n",
        "  # Otherwise, generate input for model2\n",
        "  input4sp, y_sp, y_hr = generate_scaled(input_df,user_embed,route_embed,scaler_dic)\n",
        "  # predict speed and distance\n",
        "  pred_sp,pred_hr = SpeedHr_model(input4sp)\n",
        "  # inverse transform speed and distance & combine with the stop point\n",
        "  pred_sp = scaler_dic['speed'].inverse_transform([np.array(pred_sp).flatten()]) * apply_stop(stop_point)\n",
        "  pred_hr = scaler_dic['heart_rate'].inverse_transform([np.array(pred_hr).flatten()]) * apply_stop(stop_point)\n",
        "  return pred_sp,pred_hr,pred_dis,stop_point\n",
        "\n",
        "\n",
        "\n",
        "def load_model_torch(filename, model):\n",
        "\n",
        "    DATA_PATH = \"/content/gdrive/My Drive/COMP5703/onehot/\"+filename\n",
        "    # https://drive.google.com/drive/folders/1IQ7IoQV6PZXRj0G-CKJKBTMENsYRFI86?usp=sharing\n",
        "    checkpoint = torch.load(DATA_PATH)\n",
        "    model.load_state_dict(checkpoint['best_model_state_dict'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# MLP model\n",
        "def load_distance_model(LOAD_MODEL_NAME,device):\n",
        "  class DisReg_MLP_2Layer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, p):\n",
        "        super(DisReg_MLP_2Layer, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
        "        self.fc2 = nn.Linear(hidden_dim_1, 1)\n",
        "\n",
        "        self.act_1 = nn.ReLU()\n",
        "        self.act_2 = nn.Sigmoid()\n",
        "\n",
        "        self.drop_1 = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        output = self.fc1(data)\n",
        "        output = self.act_1(output)\n",
        "        output = self.drop_1(output)\n",
        "\n",
        "        output = self.fc2(output)\n",
        "        output = self.act_2(output)\n",
        "\n",
        "        return output\n",
        "  # LOAD_MODEL_NAME = 'DisReg_MLP_2'\n",
        "  INPUT_DIM = 34\n",
        "  HIDDEN_DIM = 64\n",
        "  DROP_OUT = 0.2\n",
        "\n",
        "  model = DisReg_MLP_2Layer(\n",
        "    INPUT_DIM, HIDDEN_DIM, DROP_OUT).to(device)\n",
        "  model = load_model_torch(LOAD_MODEL_NAME, model)\n",
        "  return model\n",
        "\n",
        "def load_sphr_model(filename):\n",
        "  SpeedHr_model = load_model(filename)\n",
        "  return SpeedHr_model\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld4DlPqmzosM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443c0a26-c186-4276-bdb5-57e83dbfd59f"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "#plotly\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import joblib\n",
        "# import torch.nn as nn\n",
        "import torch\n",
        "# from sktensor import dtensor\n",
        "import pandas as pd\n",
        "# import os\n",
        "from utils import predict_sp_hr,load_dataset,load_distance_model,load_sphr_model\n",
        "# from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "from time import strftime, gmtime\n",
        "PAGE_CONFIG = {\"page_title\":\"StColab.io\",\"page_icon\":\":smiley:\",\"layout\":\"centered\"}\n",
        "st.set_page_config(**PAGE_CONFIG)\n",
        "import matplotlib\n",
        "\n",
        "# Find the exact route on the Google map using plotly\n",
        "def route_fig(df,route_option):\n",
        "  #plot route\n",
        "  longitude = eval(df[df['id']==route_option]['longitude'].values[0])\n",
        "  latitude = eval(df[df['id']==route_option]['latitude'].values[0])\n",
        "  df_new = pd.DataFrame({'longitude':longitude,'latitude':latitude}).reset_index()\n",
        "  df_new['size'] = 4\n",
        "  df_new.loc[0,'size'] = 15\n",
        "  df_new.loc[499,'size'] = 15\n",
        "  df_new['text'] = ''\n",
        "  df_new.loc[0,'text'] = 'Start'\n",
        "  df_new.loc[499,'text'] = 'End'\n",
        "  \n",
        "  # print(type(df_new))\n",
        "  token = 'pk.eyJ1IjoicHVtcGtpbmt1biIsImEiOiJja2Y2a3puN3YwMTF1MnFucnJyNGRxaTJrIn0.c9VFzhysl-4MLPibge4P3A' # You can create your own token and use it\n",
        "  fig_route = px.scatter_mapbox(df_new, lat=\"latitude\", lon=\"longitude\",color=\"index\",size='size',text='text',size_max=12,\n",
        "                                labels = {'index':'StartToEnd'},zoom = 11,color_continuous_scale=px.colors.sequential.Sunsetdark) \n",
        "  fig_route.update_layout(height=600,width=800,\n",
        "    legend = dict(font=dict(size=1)),\n",
        "    font=dict(size=24),\n",
        "      mapbox = {\n",
        "      'accesstoken': token,\n",
        "      'style': \"outdoors\"})\n",
        "  return fig_route\n",
        "\n",
        "\n",
        "\n",
        "# main\n",
        "if __name__ == '__main__':\n",
        "  # load data and model \n",
        "  if torch.cuda.is_available():      \n",
        "    device = torch.device('cuda')\n",
        "  else:\n",
        "    device = torch.device('cpu')\n",
        "  # path\n",
        "  file_path = '/content/gdrive/My Drive/COMP5703/Dataset/UI_data.csv'\n",
        "  # https://drive.google.com/file/d/1W8-eEOnEnjafqv-X1FMoozWnAFA5tVvA/view?usp=sharing\n",
        "  route_dir = '/content/gdrive/My Drive/COMP5703/embedding/routeEmbed_tensorD_13.m'\n",
        "  # https://drive.google.com/file/d/1oAvzIZEufHgEeIAUmxbwfnZmXG7Ragrt/view?usp=sharing\n",
        "  user_dir= '/content/gdrive/My Drive/COMP5703/embedding/userEmbed_tensorD_13.m'\n",
        "  # https://drive.google.com/file/d/1E8P_FqXgVzSAU-TwEFBSMU1OW694kSJu/view?usp=sharing\n",
        "  LOAD_MODEL_NAME = 'DisReg_MLP_2L_13'\n",
        "  model2_name = '/content/gdrive/My Drive/COMP5703/model/[1112]speed2hr_r13.h5'\n",
        "  # https://drive.google.com/file/d/19DzgT4-eCFUvqFo-LbAaiKqSQUuOmkhl/view?usp=sharing\n",
        "  df,user_embed_df,route_embed_df,OneHot_enc,scaler_dic = load_dataset(file_path,route_dir,user_dir)\n",
        "  Distance_model = load_distance_model(LOAD_MODEL_NAME,device)\n",
        "  SpeedHr_model = load_sphr_model(model2_name)\n",
        "  \n",
        "  ## GUI demo\n",
        "  st.title(\"IoT Personalized Fitness Recommendation System\")\n",
        "\n",
        "  # User input interface\n",
        "  st.sidebar.subheader(\"User name\")\n",
        "  user_option = st.sidebar.selectbox('',list(set(df['name'])))\n",
        "  user_id = df[df['name']==user_option]['userId'].values[0]\n",
        "  st.sidebar.subheader(\"Sport type\")\n",
        "  sport = st.sidebar.selectbox('',['run','bike','mountain bike'])\n",
        "  st.sidebar.subheader(\"Workout route\")\n",
        "  workout_city = st.sidebar.selectbox('',df[df['sport']==sport]['city'].values)\n",
        "  route_option = df[df['city']==workout_city]['id'].values[0]\n",
        "  gender = df[df['name']==user_option]['gender'].values[0]\n",
        "  recommend_cal = df[df['id']==route_option]['calories'].values[0]\n",
        "  total_distance = df[df['id']==route_option]['distance_adjusted_sum'].values[0]\n",
        "  st.subheader('Route overview on map')\n",
        "  st.plotly_chart(route_fig(df,route_option))\n",
        "  st.sidebar.subheader(\"Target calories\")\n",
        "  calories = st.sidebar.slider('',min_value = int(0.5*recommend_cal),max_value = int(2*recommend_cal),value=int(recommend_cal),step=1)\n",
        "  if st.sidebar.button('Get In. Get Fit. Get on with Life!'):\n",
        "    sp,hr,dis,stop_point = predict_sp_hr(df, user_embed_df, route_embed_df,scaler_dic,OneHot_enc,device,\n",
        "                    Distance_model, SpeedHr_model, \n",
        "                      workout_id = route_option ,user_id=user_id,gender=gender,calories=calories,sport=sport)\n",
        "    \n",
        "    # If your target is hard to achieve\n",
        "    if stop_point == -1:\n",
        "      st.warning('Warning ! The required workout distance to achieve your calories target has exceeded the maximum distance of the selected workout route. Please reduce your calories target or select a different workout route. ')\n",
        "    # Successful prediction\n",
        "    else:\n",
        "      # User input parameters summary\n",
        "      st.sidebar.success('Predicted successfully')\n",
        "      data = {\n",
        "          'user name': user_option,\n",
        "          'gender': gender,\n",
        "          'workout id': workout_city,\n",
        "          'sport_type': sport,\n",
        "          'Target calories': calories\n",
        "          }\n",
        "      features = pd.DataFrame(data, index=[0])\n",
        "      st.sidebar.subheader('User Input parameters')\n",
        "      st.sidebar.write(features)\n",
        "      st.write('The total length of the route you choose is:',total_distance, 'km')\n",
        "      st.write('You need to exercise', round(dis, 2), 'km to achieve your fitness goals！')\n",
        "\n",
        "      st.subheader('Predicted speed')\n",
        "      dis_ls = df[df['id']==route_option]['derived_distance']\n",
        "      dis_ls = eval(dis_ls.values[0])\n",
        "      sp_ls = (np.array(sp.flatten())).tolist()\n",
        "      hr_ls = (np.array(hr.flatten())).tolist()\n",
        "\n",
        "      # find sequence length\n",
        "      seq_len = len(hr_ls)\n",
        "      # remove additional points from distance sequence\n",
        "      dis_ls = dis_ls[0:seq_len]\n",
        "      # compute minutes sequence\n",
        "      min_ls = [(dis/sp_ls[idx])*60 if sp_ls[idx] != 0 else 0 for idx, dis in enumerate(dis_ls)]\n",
        "      # interval to present in minutes\n",
        "      interval = 5\n",
        "      # new resampled minutes list\n",
        "      min_new = []\n",
        "      # original indices of each resampled chunck\n",
        "      idx_ls = []\n",
        "      start_idx = 0\n",
        "      end_idx = 1\n",
        "      # iterate over min_ls to resample data based on desired interval\n",
        "      while end_idx <= seq_len:\n",
        "          sum_min = sum(min_ls[start_idx:end_idx])\n",
        "\n",
        "          if sum_min < 5 and end_idx < seq_len:\n",
        "              end_idx += 1\n",
        "\n",
        "          elif end_idx < seq_len:\n",
        "              min_new.append(sum_min)\n",
        "              idx_ls.append([start_idx, end_idx])\n",
        "              start_idx = end_idx\n",
        "              end_idx += 1\n",
        "\n",
        "          else:\n",
        "              # min_new.append(sum_min)\n",
        "              # idx_ls.append([start_idx, end_idx])\n",
        "              start_idx = end_idx\n",
        "              end_idx += 1\n",
        "\n",
        "      # compute new speed list based on resampled indices\n",
        "      sp_new = [sum(dis_ls[indices[0]:indices[1]])/(min_new[i]/60) for i, indices in enumerate(idx_ls)]\n",
        "      # round resampled minutes nearest number\n",
        "      min_new_adjusted = [min//interval*interval for min in min_new]\n",
        "\n",
        "      ## keep last item because it may not be a full interval period\n",
        "      # min_new_adjusted[-1] = min_new[-1]\n",
        "\n",
        "      # cumulative sum of resampled minutes list to be x axis\n",
        "      min_new_adjusted_sum = list(np.cumsum(min_new_adjusted))\n",
        "\n",
        "      # function to format minutes\n",
        "      def format_time(min):\n",
        "          return str(strftime(\"%H:%M:%S\", gmtime(min*60)))\n",
        "\n",
        "      # format x axis\n",
        "      min_label = [format_time(min_new_adjusted_sum[i-1])+'-'+format_time(min_new_adjusted_sum[i]) if i>0 else format_time(0)+'-'+format_time(min_new_adjusted_sum[i]) for i in range(len(min_new_adjusted_sum))]\n",
        "\n",
        "      # waterfall plot base value\n",
        "      base = min(sp_new)\n",
        "      # compute waterfall plot speed list based on base value\n",
        "      sp_waterfall = [sp_new[i] - sp_new[i-1] if i>0 else sp_new[0]-base for i, _ in enumerate(sp_new)]\n",
        "      # compute waterfall plot data labels\n",
        "      sp_waterfall_label = ['+' + str(round(sp, 3)) if sp>0 else str(round(sp, 3)) for sp in sp_waterfall]\n",
        "      # waterfall plot starting point should not be delta\n",
        "      sp_waterfall_label[0] = str(round(sp_new[0], 3))\n",
        "\n",
        "      # Speed line plot\n",
        "      x_axis =np.arange(1,499)\n",
        "      fig_sp = go.Figure()\n",
        "      fig_sp.add_trace(go.Scatter(x=x_axis, y=sp.flatten(),\n",
        "                    mode='lines'))\n",
        "      fig_sp.update_layout(font_size=16,height=600,width=1400,\n",
        "      xaxis = dict(title = 'TimeStamp'),\n",
        "      yaxis = dict(title = 'Speed(km/h)'))\n",
        "      st.plotly_chart(fig_sp)\n",
        "\n",
        "      # Speed waterfall plot\n",
        "      st.subheader('Speed recommendation')\n",
        "      fig_sp_interval = go.Figure(go.Waterfall(\n",
        "      orientation = 'v',\n",
        "      x = min_label,\n",
        "      textposition = 'outside',\n",
        "      text = sp_waterfall_label,\n",
        "      y = sp_waterfall,\n",
        "      base = base,\n",
        "      decreasing = {'marker':{'color': '#00bc12'}},\n",
        "      increasing = {'marker':{'color': '#ff7500'}},\n",
        "      connector = {'line':{'color': 'rgb(63, 63, 63)', 'width': 3}},\n",
        "      ))\n",
        "\n",
        "      fig_sp_interval.update_layout(font_size=16,height=600,width=1400,\n",
        "              # title = 'Workout Speed Recommendation',\n",
        "              xaxis_title= 'Time Elapsed (min)',\n",
        "              yaxis_title= 'Speed (km/h)',\n",
        "              yaxis=dict(range=[min(sp_new)*0.85, max(sp_new)*1.05])\n",
        "      )\n",
        "\n",
        "      st.plotly_chart(fig_sp_interval)\n",
        "\n",
        "      # Heart-rate line plot\n",
        "      st.subheader('Predicted heart rate')\n",
        "      x_axis_hr = np.arange(1,stop_point)\n",
        "      fig_hr = go.Figure()\n",
        "      fig_hr.add_trace(go.Scatter(x=x_axis_hr, y=hr.flatten(),\n",
        "                    mode='lines'))\n",
        "      fig_hr.update_layout(font_size=16,height=600,width=1400,\n",
        "      xaxis = dict(title = 'TimeStamp'),\n",
        "      yaxis = dict(title = 'HeartRate(bpm)'))\n",
        "      st.plotly_chart(fig_hr)\n",
        "\n",
        "      # Heart-rate box plot\n",
        "      st.subheader('Heart rate prediction')\n",
        "      hr_new = [hr_ls[indices[0]:indices[1]] for indices in idx_ls]\n",
        "      fig_hr_interval = go.Figure()\n",
        "\n",
        "      # initialise the Matplotlib cmap & normalizer for boxplot colors\n",
        "      norm = matplotlib.colors.Normalize(vmin=min(hr_ls), vmax=max(hr_ls))\n",
        "      cmap = matplotlib.cm.get_cmap('RdYlBu')\n",
        "\n",
        "      for i, hr in enumerate(hr_new):\n",
        "\n",
        "          # find the median of each hr batch and use the normalizer to map it to an \n",
        "          # appropriate color based on the median value\n",
        "          median = np.median(hr) # find the median\n",
        "          color = 'rgb' + str(cmap(norm(median))[0:3]) # normalise\n",
        "          \n",
        "          fig_hr_interval.add_trace(go.Box(\n",
        "              y=hr, \n",
        "              name=min_label[i], \n",
        "              fillcolor=color,\n",
        "              line=dict(color='black')\n",
        "              ))\n",
        "\n",
        "      fig_hr_interval.update_layout(font_size=16,height=600,width=1400,\n",
        "              # title = 'Workout Heart Rate Prediction',\n",
        "              xaxis_title= 'Time Elapsed (min)',\n",
        "              yaxis_title= 'Heart Rate (bpm)',\n",
        "              showlegend = False\n",
        "      )\n",
        "\n",
        "      st.plotly_chart(fig_hr_interval)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXYb-Z_Sbxi1"
      },
      "source": [
        "# Connection of Streamlit with URL by ngrok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUXYFM2NJu2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5af52a5-8c7b-4048-acfd-abd63e52a889"
      },
      "source": [
        "# Check if the file has been written to the current Colab\r\n",
        "!ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "app.py\tgdrive\tsample_data  utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWd0JDiYJw_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1978348d-8e17-45e9-fe2c-a620e1e58508"
      },
      "source": [
        "!ngrok"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NAME:\n",
            "   ngrok - tunnel local ports to public URLs and inspect traffic\n",
            "\n",
            "DESCRIPTION:\n",
            "    ngrok exposes local networked services behinds NATs and firewalls to the\n",
            "    public internet over a secure tunnel. Share local websites, build/test\n",
            "    webhook consumers and self-host personal services.\n",
            "    Detailed help for each command is available with 'ngrok help <command>'.\n",
            "    Open http://localhost:4040 for ngrok's web interface to inspect traffic.\n",
            "\n",
            "EXAMPLES:\n",
            "    ngrok http 80                    # secure public URL for port 80 web server\n",
            "    ngrok http -subdomain=baz 8080   # port 8080 available at baz.ngrok.io\n",
            "    ngrok http foo.dev:80            # tunnel to host:port instead of localhost\n",
            "    ngrok http https://localhost     # expose a local https server\n",
            "    ngrok tcp 22                     # tunnel arbitrary TCP traffic to port 22\n",
            "    ngrok tls -hostname=foo.com 443  # TLS traffic for foo.com to port 443\n",
            "    ngrok start foo bar baz          # start tunnels from the configuration file\n",
            "\n",
            "VERSION:\n",
            "   2.3.35\n",
            "\n",
            "AUTHOR:\n",
            "  inconshreveable - <alan@ngrok.com>\n",
            "\n",
            "COMMANDS:\n",
            "   authtoken\tsave authtoken to configuration file\n",
            "   credits\tprints author and licensing information\n",
            "   http\t\tstart an HTTP tunnel\n",
            "   start\tstart tunnels by name from the configuration file\n",
            "   tcp\t\tstart a TCP tunnel\n",
            "   tls\t\tstart a TLS tunnel\n",
            "   update\tupdate ngrok to the latest version\n",
            "   version\tprint the version string\n",
            "   help\t\tShows a list of commands or help for one command\n",
            "\n",
            "PYNGROK VERSION:\n",
            "   4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNOdAVBxr3-P"
      },
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPPAPrjJr_e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2302110-ecb0-4b45-c33b-f632b871235f"
      },
      "source": [
        "!pgrep streamlit"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV0TAVH7sCKb"
      },
      "source": [
        "# Use pyngrok and pass in the port of Streamlit (i.e. 8501)\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(port='8501')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gRoyysEsEQA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "82f8d362-9bef-48e7-fecf-42fcebb0d368"
      },
      "source": [
        "public_url"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://a955489572d4.ngrok.io'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwYY-JPSy4Mc"
      },
      "source": [
        "# Stop it\r\n",
        "!kill 741"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}